{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7feb916-242a-4212-8d84-7a31e7cdeae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/26 16:29:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "078fc6be-1505-4c02-bb5d-7f984f8d5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read  the data from  dbfs:/databricks-datasets/adult/adult.data  into Saprk dataframe \n",
    "file_path= 'file:////Users/kaustuvkunal/Learning/LTI_Mindtree/PySpark_CodingQuestion1/data/employee.csv'\n",
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferschema\",\"True\").load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c68339c-6cb6-453e-a115-8cf53547c636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(employee_id=1, first_name='John', last_name='Doe', salary=50000),\n",
       " Row(employee_id=2, first_name='Jane', last_name='Smith', salary=60000),\n",
       " Row(employee_id=3, first_name='Bob', last_name='Johnson', salary=75000),\n",
       " Row(employee_id=4, first_name='Alice', last_name='Williams', salary=55000),\n",
       " Row(employee_id=5, first_name='Charlie', last_name='Brown', salary=80000)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d356e2-a1b0-48da-a94c-99752f0891e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+\n",
      "|employee_id|first_name|last_name|salary|\n",
      "+-----------+----------+---------+------+\n",
      "|          1|      John|      Doe| 50000|\n",
      "|          2|      Jane|    Smith| 60000|\n",
      "|          3|       Bob|  Johnson| 75000|\n",
      "|          4|     Alice| Williams| 55000|\n",
      "|          5|   Charlie|    Brown| 80000|\n",
      "+-----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ac39c5a-4f93-42a4-8938-199d3503d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary SQL table\n",
    "df.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c16914e6-5dcb-4fd3-881f-f1370d28bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to get the average salary of employees\n",
    "average_salary_query = \"SELECT AVG(salary) as avg_salary FROM employees\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dade2d4-4a4f-452b-ae44-8b6c2d2fe6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SQL query and store the result in a DataFrame\n",
    "result_df = spark.sql(average_salary_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9be8fd4-ddf1-45b1-a51c-bfc742af880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg_salary|\n",
      "+----------+\n",
      "|   64000.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fffc4f30-b7e4-4fb7-ae3a-1a0a52163c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25c3ce84-a881-4e47-8797-2b5cbbcd2c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64000.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "employee.select(avg(\"salary\").alias(\"avg_salary\")).first()[\"avg_salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd62b2-c5ca-4ef0-8c99-5ead3128de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "\n",
    "employee.select(avg(coalesce(employee[\"salary\"], 0))).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcd02788-0c97-4358-800d-ebf95c78b9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64000.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, coalesce, lit\n",
    "df=employee\n",
    "employee.select(avg(coalesce(employee[\"salary\"], lit(0)))).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be9b057b-35be-47ae-bb83-d350e673b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyArrow\n",
      "  Obtaining dependency information for PyArrow from https://files.pythonhosted.org/packages/9d/7e/026be3d56c930bfc3557a42baebfe7c10c4e32ea7db7861f4b40e54adf1e/pyarrow-13.0.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from PyArrow) (1.25.2)\n",
      "Downloading pyarrow-13.0.0-cp311-cp311-macosx_10_14_x86_64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyArrow\n",
      "Successfully installed PyArrow-13.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ecbe0c7-e3a1-464d-9e8c-4da17627dce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47000.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"John\", \"Doe\", 50000),\n",
    "                     (\"Jane\", \"Smith\", 60000),\n",
    "                     (\"Bob\", \"Johnson\", 75000),\n",
    "                     (\"Alice\", \"Williams\", 55000),\n",
    "                     (\"Charlie\", \"Brown\", 80000),\n",
    "                     (\"Eve\", \"Taylor\", None),\n",
    "                     (\"Frank\", \"Wilson\", -10000),\n",
    "                     (\"Grace\", \"Adams\", 90000),\n",
    "                     (\"Harry\", \"Clark\", 40000),\n",
    "                     (\"Ivy\", \"Davis\", 30000)]\n",
    "schema = [\"first_name\", \"last_name\", \"salary\"]\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "\n",
    "df.select(avg(coalesce(df[\"salary\"], lit(0)))).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8314c6-b788-48da-b75b-d5aef9249c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
